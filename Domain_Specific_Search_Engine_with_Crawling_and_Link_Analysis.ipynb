{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a Domain-Specific Search Engine with Crawling and Link Analysis"
      ],
      "metadata": {
        "id": "uWaBTdC1Ok5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAIN = \"machine learning\""
      ],
      "metadata": {
        "id": "jz1fGOOYNjg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_urls = [\n",
        "    \"https://openai.com\",\n",
        "    \"https://deepmind.com\",\n",
        "    \"https://arxiv.org\",\n",
        "    \"https://kaggle.com\",\n",
        "    \"https://towardsdatascience.com\",\n",
        "    \"https://machinelearningmastery.com\",\n",
        "    \"https://distill.pub\",\n",
        "    \"https://ai.google\",\n",
        "    \"https://huggingface.co\",\n",
        "    \"https://paperswithcode.com\",\n",
        "    \"https://developer.nvidia.com/deep-learning\",\n",
        "    \"https://colah.github.io\",\n",
        "    \"https://ruder.io\",\n",
        "    \"https://jalammar.github.io\"\n",
        "]"
      ],
      "metadata": {
        "id": "qE7HGdkKNl1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KlSAMsg6Oj1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import networkx as nx\n",
        "\n",
        "def normalize_url(url):\n",
        "    \"\"\"Normalize URL by removing fragments and query parameters\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return parsed.scheme + \"://\" + parsed.netloc + parsed.path\n",
        "\n",
        "def get_domain(url):\n",
        "    return urlparse(url).netloc\n",
        "\n",
        "def crawl(start_urls, max_pages=100, max_visits_per_domain=20, delay=1.5, timeout=10):\n",
        "    visited = set()\n",
        "    to_visit = [url.strip() for url in start_urls if url.strip()]\n",
        "    inverted_index = defaultdict(set)\n",
        "    web_connection = {}\n",
        "    domain_visits = defaultdict(int)\n",
        "    last_request_time = {}\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; AcademicCrawler/1.0)'}\n",
        "    seen = set(to_visit)\n",
        "\n",
        "    print(f\"Starting crawl with {len(to_visit)} seed URLs\")\n",
        "\n",
        "    while to_visit and len(visited) < max_pages:\n",
        "        url = to_visit.pop(0)\n",
        "\n",
        "        try:\n",
        "            # Normalize URL\n",
        "            url = normalize_url(url)\n",
        "            domain = get_domain(url)\n",
        "\n",
        "            # Check domain limits\n",
        "            if domain_visits[domain] >= max_visits_per_domain:\n",
        "                continue\n",
        "\n",
        "            # Respect crawl delay\n",
        "            current_time = time.time()\n",
        "            if domain in last_request_time:\n",
        "                elapsed = current_time - last_request_time[domain]\n",
        "                if elapsed < delay:\n",
        "                    time.sleep(delay - elapsed)\n",
        "\n",
        "            # Fetch page\n",
        "            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
        "            last_request_time[domain] = time.time()\n",
        "\n",
        "            # Check content type\n",
        "            content_type = response.headers.get('Content-Type', '').lower()\n",
        "            if 'text/html' not in content_type:\n",
        "                continue\n",
        "\n",
        "            # Parse HTML\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            visited.add(url)\n",
        "            domain_visits[domain] += 1\n",
        "\n",
        "            # Extract text (remove JavaScript/CSS)\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.extract()\n",
        "            text = soup.get_text()\n",
        "            words = re.findall(r'\\b\\w{3,20}\\b', text.lower())  # Only words of 3-20 chars\n",
        "\n",
        "            # Update inverted index\n",
        "            for word in set(words):\n",
        "                inverted_index[word].add(url)\n",
        "\n",
        "            # Extract links\n",
        "            links = []\n",
        "            for a in soup.find_all('a', href=True):\n",
        "                href = a['href'].strip()\n",
        "                if href.startswith('javascript:') or href.startswith('mailto:'):\n",
        "                    continue\n",
        "\n",
        "                absolute_url = urljoin(url, href)\n",
        "                absolute_url = normalize_url(absolute_url)\n",
        "                parsed = urlparse(absolute_url)\n",
        "\n",
        "                # Validate URL\n",
        "                if parsed.scheme in ('http', 'https') and parsed.netloc:\n",
        "                    links.append(absolute_url)\n",
        "                    if absolute_url not in seen and absolute_url not in visited:\n",
        "                        seen.add(absolute_url)\n",
        "                        to_visit.append(absolute_url)\n",
        "\n",
        "            # Update web graph\n",
        "            web_connection[url] = links\n",
        "            print(f\"Crawled: {url} (Links: {len(links)}, Words: {len(words)})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error crawling {url}: {str(e)[:100]}\")\n",
        "\n",
        "    print(f\"\\nCrawl completed! Pages: {len(visited)}, Domains: {len(domain_visits)}, Terms: {len(inverted_index)}\")\n",
        "    return dict(inverted_index), web_connection\n",
        "\n",
        "# Run crawler\n",
        "inverted_index, web_connection = crawl(\n",
        "    seed_urls,\n",
        "    max_pages=100,\n",
        "    max_visits_per_domain=20,\n",
        "    delay=1.5,\n",
        "    timeout=15\n",
        ")\n",
        "\n",
        "# Save results\n",
        "with open('inverted_index.pkl', 'wb') as f:\n",
        "    pickle.dump(inverted_index, f)\n",
        "\n",
        "with open('web_connection.pkl', 'wb') as f:\n",
        "    pickle.dump(web_connection, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UEAvIWANqJH",
        "outputId": "2de1d904-fc2c-4627-d45f-00d55cc85b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting crawl with 14 seed URLs\n",
            "Crawled: https://openai.com (Links: 0, Words: 5)\n",
            "Crawled: https://deepmind.com (Links: 199, Words: 1155)\n",
            "Crawled: https://arxiv.org (Links: 260, Words: 695)\n",
            "Crawled: https://kaggle.com (Links: 0, Words: 8)\n",
            "Crawled: https://towardsdatascience.com (Links: 107, Words: 723)\n",
            "Crawled: https://machinelearningmastery.com (Links: 0, Words: 7)\n",
            "Crawled: https://distill.pub (Links: 62, Words: 945)\n",
            "Crawled: https://ai.google (Links: 260, Words: 2137)\n",
            "Crawled: https://huggingface.co (Links: 80, Words: 500)\n",
            "Crawled: https://paperswithcode.com (Links: 103, Words: 530)\n",
            "Crawled: https://developer.nvidia.com/deep-learning (Links: 22, Words: 704)\n",
            "Crawled: https://colah.github.io (Links: 65, Words: 501)\n",
            "Crawled: https://ruder.io (Links: 5, Words: 89)\n",
            "Crawled: https://jalammar.github.io (Links: 250, Words: 3924)\n",
            "Crawled: https://deepmind.com/models/ (Links: 186, Words: 950)\n",
            "Crawled: https://deepmind.com/models/gemini/pro/ (Links: 192, Words: 2450)\n",
            "Crawled: https://deepmind.com/models/gemini/flash/ (Links: 190, Words: 2188)\n",
            "Crawled: https://deepmind.com/models/gemini/flash-lite/ (Links: 188, Words: 2071)\n",
            "Crawled: https://deepmind.com/models/gemini/ (Links: 184, Words: 1509)\n",
            "Crawled: https://deepmind.com/models/gemma/gemma-3/ (Links: 204, Words: 1659)\n",
            "Crawled: https://deepmind.com/models/gemma/gemma-3n/ (Links: 201, Words: 1360)\n",
            "Crawled: https://deepmind.com/models/gemma/shieldgemma-2/ (Links: 198, Words: 1292)\n",
            "Crawled: https://deepmind.com/models/gemma/ (Links: 191, Words: 1173)\n",
            "Crawled: https://deepmind.com/models/imagen/ (Links: 172, Words: 8308)\n",
            "Crawled: https://deepmind.com/models/lyria/ (Links: 189, Words: 1477)\n",
            "Crawled: https://deepmind.com/models/veo/ (Links: 177, Words: 4464)\n",
            "Crawled: https://deepmind.com/models/project-astra/ (Links: 167, Words: 1982)\n",
            "Crawled: https://deepmind.com/models/project-mariner/ (Links: 167, Words: 1084)\n",
            "Crawled: https://deepmind.com/models/gemini-diffusion/ (Links: 162, Words: 1047)\n",
            "Crawled: https://deepmind.com/research/ (Links: 179, Words: 1276)\n",
            "Crawled: https://deepmind.com/research/projects/ (Links: 169, Words: 951)\n",
            "Crawled: https://deepmind.com/research/publications/ (Links: 180, Words: 1448)\n",
            "Crawled: https://deepmind.com/discover/blog/ (Links: 179, Words: 1156)\n",
            "Crawled: https://deepmind.google.com/science/weatherlab/ (Links: 6, Words: 52)\n",
            "Crawled: https://ai.google/advancing-ai/milestones/ (Links: 178, Words: 1290)\n",
            "Crawled: https://aistudio.google.com/prompts/new_chat (Links: 6, Words: 52)\n",
            "Crawled: https://gemini.google.com/ (Links: 1, Words: 2)\n",
            "Crawled: https://ai.google/ (Links: 260, Words: 2137)\n",
            "Crawled: https://deepmind.google (Links: 199, Words: 1155)\n",
            "Crawled: https://labs.google/ (Links: 81, Words: 896)\n",
            "Crawled: https://research.google (Links: 123, Words: 990)\n",
            "Crawled: http://labs.google/flow (Links: 19, Words: 195)\n",
            "Crawled: https://cloud.google.com/vertex-ai (Links: 919, Words: 5462)\n",
            "Crawled: https://gemini.google.com/veo (Links: 1, Words: 2)\n",
            "Crawled: https://aistudio.google.com/generate-video (Links: 6, Words: 52)\n",
            "Crawled: https://labs.google/fx/tools/whisk (Links: 0, Words: 3)\n",
            "Crawled: https://aistudio.google.com/gen-media (Links: 6, Words: 52)\n",
            "Crawled: https://labs.google/fx/tools/music-fx-dj (Links: 0, Words: 6)\n",
            "Crawled: https://aistudio.google.com/app/apps/bundled/promptdj-midi (Links: 6, Words: 52)\n",
            "Crawled: https://x.com/googledeepmind (Links: 6, Words: 33)\n",
            "Crawled: https://www.instagram.com/googledeepmind (Links: 0, Words: 0)\n",
            "Crawled: https://www.youtube.com/@googledeepmind (Links: 15, Words: 12)\n",
            "Crawled: https://www.linkedin.com/company/googledeepmind/ (Links: 252, Words: 1626)\n",
            "Crawled: https://github.com/google-deepmind (Links: 191, Words: 677)\n",
            "Crawled: https://policies.google.com/privacy (Links: 278, Words: 8262)\n",
            "Crawled: https://www.google.com (Links: 30, Words: 12)\n",
            "Crawled: https://about.google/ (Links: 45, Words: 221)\n",
            "Crawled: https://about.google/products/ (Links: 174, Words: 575)\n",
            "Crawled: https://policies.google.com/terms (Links: 150, Words: 3493)\n",
            "Crawled: https://www.cornell.edu/ (Links: 153, Words: 591)\n",
            "Crawled: https://info.arxiv.org/about/ourmembers.html (Links: 150, Words: 1945)\n",
            "Crawled: https://info.arxiv.org/about/donate.html (Links: 142, Words: 576)\n",
            "Crawled: https://arxiv.org/IgnoreMe (Links: 0, Words: 22)\n",
            "Crawled: https://arxiv.org/login (Links: 21, Words: 125)\n",
            "Crawled: https://info.arxiv.org/help (Links: 188, Words: 670)\n",
            "Crawled: https://arxiv.org/search/advanced (Links: 21, Words: 520)\n",
            "Crawled: https://arxiv.org/ (Links: 260, Words: 695)\n",
            "Crawled: https://info.arxiv.org/about (Links: 187, Words: 831)\n",
            "Crawled: https://arxiv.org/archive/astro-ph (Links: 80, Words: 473)\n",
            "Crawled: https://arxiv.org/list/astro-ph/new (Links: 1999, Words: 37712)\n",
            "Crawled: https://arxiv.org/list/astro-ph/recent (Links: 635, Words: 2551)\n",
            "Crawled: https://arxiv.org/search/astro-ph (Links: 22, Words: 385)\n",
            "Crawled: https://arxiv.org/list/astro-ph.GA/recent (Links: 839, Words: 2804)\n",
            "Crawled: https://arxiv.org/list/astro-ph.CO/recent (Links: 824, Words: 2823)\n",
            "Crawled: https://arxiv.org/list/astro-ph.EP/recent (Links: 916, Words: 3337)\n",
            "Crawled: https://arxiv.org/list/astro-ph.HE/recent (Links: 516, Words: 2559)\n",
            "Crawled: https://arxiv.org/list/astro-ph.IM/recent (Links: 997, Words: 3424)\n",
            "Crawled: https://arxiv.org/list/astro-ph.SR/recent (Links: 677, Words: 2876)\n",
            "Crawled: https://arxiv.org/archive/cond-mat (Links: 89, Words: 458)\n",
            "Crawled: https://arxiv.org/list/cond-mat/new (Links: 1747, Words: 37617)\n",
            "Crawled: https://arxiv.org/list/cond-mat/recent (Links: 487, Words: 2208)\n",
            "Crawled: https://arxiv.org/search/cond-mat (Links: 22, Words: 386)\n",
            "Crawled: https://arxiv.org/list/cond-mat.dis-nn/recent (Links: 209, Words: 1254)\n",
            "Crawled: https://info.arxiv.org/help/math/index.html (Links: 144, Words: 686)\n",
            "Crawled: https://info.arxiv.org/help/cs/index.html (Links: 145, Words: 688)\n",
            "Crawled: https://info.arxiv.org/help/q-bio/index.html (Links: 143, Words: 588)\n",
            "Crawled: https://info.arxiv.org/help/q-fin/index.html (Links: 143, Words: 633)\n",
            "Crawled: https://info.arxiv.org/help/stat/index.html (Links: 140, Words: 511)\n",
            "Crawled: https://info.arxiv.org/help/eess/index.html (Links: 143, Words: 617)\n",
            "Crawled: https://info.arxiv.org/help/econ/index.html (Links: 143, Words: 610)\n",
            "Crawled: https://info.arxiv.org/help/submit/index.html (Links: 203, Words: 1690)\n",
            "Crawled: https://info.arxiv.org/about/people/index.html (Links: 154, Words: 562)\n",
            "Crawled: https://info.arxiv.org/help/contact.html (Links: 154, Words: 774)\n",
            "Crawled: https://info.arxiv.org/help/subscribe (Links: 135, Words: 487)\n",
            "Crawled: https://info.arxiv.org/help/license/index.html (Links: 172, Words: 1205)\n",
            "Crawled: https://info.arxiv.org/help/policies/privacy_policy.html (Links: 180, Words: 3409)\n",
            "Crawled: https://info.arxiv.org/help/web_accessibility.html (Links: 138, Words: 548)\n",
            "Crawled: https://status.arxiv.org (Links: 18, Words: 91)\n",
            "Crawled: https://subscribe.sorryapp.com/24846f03/email/new (Links: 2, Words: 61)\n",
            "Crawled: https://subscribe.sorryapp.com/24846f03/slack/new (Links: 2, Words: 60)\n",
            "\n",
            "Crawl completed! Pages: 100, Domains: 33, Terms: 20489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph(web_connection):\n",
        "    G = nx.DiGraph()\n",
        "    for url, links in web_connection.items():\n",
        "        G.add_node(url)\n",
        "        for link in links:\n",
        "            if link in web_connection:  # Only include crawled pages\n",
        "                G.add_edge(url, link)\n",
        "    return G\n",
        "\n",
        "# Load data\n",
        "with open('web_connection.pkl', 'rb') as f:\n",
        "    web_connection = pickle.load(f)\n",
        "\n",
        "web_graph = build_graph(web_connection)\n",
        "print(f\"Web graph: Nodes={web_graph.number_of_nodes()}, Edges={web_graph.number_of_edges()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJJRgLo-NsGE",
        "outputId": "7ead0103-b4cc-4bb8-d54c-edabadcce0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Web graph: Nodes=100, Edges=1449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pagerank(graph, damping=0.85, max_iter=100, tol=1e-6):\n",
        "    nodes = list(graph.nodes())\n",
        "    n = len(nodes)\n",
        "    node_index = {node: i for i, node in enumerate(nodes)}\n",
        "    pr = np.ones(n) / n\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adj_matrix = np.zeros((n, n))\n",
        "    out_degrees = np.zeros(n)\n",
        "\n",
        "    for i, node in enumerate(nodes):\n",
        "        successors = list(graph.successors(node))\n",
        "        out_degree = len(successors)\n",
        "        out_degrees[i] = out_degree\n",
        "        if out_degree > 0:\n",
        "            for succ in successors:\n",
        "                if succ in node_index:\n",
        "                    j = node_index[succ]\n",
        "                    adj_matrix[j, i] = 1 / out_degree\n",
        "\n",
        "    # Handle dangling nodes\n",
        "    dangling_nodes = np.where(out_degrees == 0)[0]\n",
        "    dangling_weights = np.ones(n) / n if len(dangling_nodes) > 0 else None\n",
        "\n",
        "    # Power iteration\n",
        "    for _ in range(max_iter):\n",
        "        new_pr = np.zeros(n)\n",
        "\n",
        "        # Multiply with adjacency matrix\n",
        "        new_pr = damping * np.dot(adj_matrix, pr)\n",
        "\n",
        "        # Add damping factor\n",
        "        new_pr += (1 - damping) / n\n",
        "\n",
        "        # Add dangling nodes contribution\n",
        "        if dangling_weights is not None:\n",
        "            new_pr += damping * pr[dangling_nodes].sum() * dangling_weights\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm(new_pr - pr, 1) < tol:\n",
        "            break\n",
        "        pr = new_pr\n",
        "\n",
        "    return {node: pr[i] for i, node in enumerate(nodes)}\n",
        "\n",
        "def hits(graph, max_iter=50, tol=1e-6):\n",
        "    nodes = list(graph.nodes())\n",
        "    n = len(nodes)\n",
        "    auth = np.ones(n)\n",
        "    hub = np.ones(n)\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adj_matrix = np.zeros((n, n))\n",
        "    for i, node in enumerate(nodes):\n",
        "        for successor in graph.successors(node):\n",
        "            if successor in nodes:\n",
        "                j = nodes.index(successor)\n",
        "                adj_matrix[i, j] = 1\n",
        "\n",
        "    # HITS iteration\n",
        "    for _ in range(max_iter):\n",
        "        new_auth = np.dot(adj_matrix.T, hub)\n",
        "        new_hub = np.dot(adj_matrix, new_auth)\n",
        "\n",
        "        # Normalize\n",
        "        auth_norm = np.linalg.norm(new_auth)\n",
        "        hub_norm = np.linalg.norm(new_hub)\n",
        "        if auth_norm > 0:\n",
        "            new_auth /= auth_norm\n",
        "        if hub_norm > 0:\n",
        "            new_hub /= hub_norm\n",
        "\n",
        "        # Check convergence\n",
        "        auth_diff = np.linalg.norm(new_auth - auth)\n",
        "        hub_diff = np.linalg.norm(new_hub - hub)\n",
        "        if auth_diff < tol and hub_diff < tol:\n",
        "            break\n",
        "\n",
        "        auth, hub = new_auth, new_hub\n",
        "\n",
        "    return (\n",
        "        {node: auth[i] for i, node in enumerate(nodes)},\n",
        "        {node: hub[i] for i, node in enumerate(nodes)}\n",
        "    )\n",
        "\n",
        "# Compute scores\n",
        "pagerank_scores = pagerank(web_graph)\n",
        "auth_scores, hub_scores = hits(web_graph)\n",
        "\n",
        "print(\"PageRank top 5:\")\n",
        "for url, score in sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print(f\"{score:.4f}: {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNyzgK-YN3Od",
        "outputId": "a2b4dbfe-7f20-4ce8-f20f-cf0b61fcbb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank top 5:\n",
            "0.0531: https://policies.google.com/terms\n",
            "0.0502: https://policies.google.com/privacy\n",
            "0.0468: https://about.google/\n",
            "0.0407: https://www.cornell.edu/\n",
            "0.0358: https://www.linkedin.com/company/googledeepmind/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def search(query, inverted_index, rank_scores, top_k=10):\n",
        "    # Tokenize query\n",
        "    words = re.findall(r'\\b\\w{3,20}\\b', query.lower())\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    # Find relevant pages\n",
        "    page_scores = defaultdict(float)\n",
        "    for word in words:\n",
        "        if word in inverted_index:\n",
        "            for url in inverted_index[word]:\n",
        "                page_scores[url] += rank_scores.get(url, 0)\n",
        "\n",
        "    # Rank by combined score\n",
        "    ranked_results = sorted(\n",
        "        [(url, score) for url, score in page_scores.items()],\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    return ranked_results[:top_k]\n",
        "\n",
        "# Load data\n",
        "with open('inverted_index.pkl', 'rb') as f:\n",
        "    inverted_index = pickle.load(f)\n",
        "\n",
        "# Sample queries\n",
        "queries = [\n",
        "    \"neural networks\",\n",
        "    \"transformer models\",\n",
        "    \"reinforcement learning\",\n",
        "    \"natural language processing\"\n",
        "]\n",
        "\n",
        "print(\"\\nSearch Results:\")\n",
        "for query in queries:\n",
        "    print(f\"\\nQuery: '{query}' (PageRank ranking)\")\n",
        "    results = search(query, inverted_index, pagerank_scores)\n",
        "    for i, (url, score) in enumerate(results):\n",
        "        print(f\"{i+1}. {score:.4f}: {url}\")\n",
        "\n",
        "    print(f\"\\nQuery: '{query}' (HITS authority ranking)\")\n",
        "    results = search(query, inverted_index, auth_scores)\n",
        "    for i, (url, score) in enumerate(results):\n",
        "        print(f\"{i+1}. {score:.4f}: {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfwUR8SwN4FH",
        "outputId": "add4f04b-7b7a-4d7c-c498-b28a25b5bcdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search Results:\n",
            "\n",
            "Query: 'neural networks' (PageRank ranking)\n",
            "1. 0.0716: https://github.com/google-deepmind\n",
            "2. 0.0447: https://arxiv.org\n",
            "3. 0.0407: https://www.cornell.edu/\n",
            "4. 0.0381: https://developer.nvidia.com/deep-learning\n",
            "5. 0.0271: https://info.arxiv.org/help/policies/privacy_policy.html\n",
            "6. 0.0188: https://arxiv.org/\n",
            "7. 0.0080: https://arxiv.org/list/cond-mat/new\n",
            "8. 0.0080: https://arxiv.org/list/cond-mat/recent\n",
            "9. 0.0079: https://arxiv.org/list/astro-ph/new\n",
            "10. 0.0079: https://arxiv.org/list/astro-ph/recent\n",
            "\n",
            "Query: 'neural networks' (HITS authority ranking)\n",
            "1. 0.3280: https://github.com/google-deepmind\n",
            "2. 0.0411: https://cloud.google.com/vertex-ai\n",
            "3. 0.0000: https://arxiv.org/\n",
            "4. 0.0000: https://info.arxiv.org/help/policies/privacy_policy.html\n",
            "5. 0.0000: https://arxiv.org\n",
            "6. 0.0000: https://www.cornell.edu/\n",
            "7. 0.0000: https://arxiv.org/list/astro-ph/new\n",
            "8. 0.0000: https://arxiv.org/list/astro-ph/recent\n",
            "9. 0.0000: https://arxiv.org/list/cond-mat/new\n",
            "10. 0.0000: https://arxiv.org/list/cond-mat/recent\n",
            "\n",
            "Query: 'transformer models' (PageRank ranking)\n",
            "1. 0.0531: https://policies.google.com/terms\n",
            "2. 0.0502: https://policies.google.com/privacy\n",
            "3. 0.0468: https://about.google/\n",
            "4. 0.0358: https://www.linkedin.com/company/googledeepmind/\n",
            "5. 0.0358: https://github.com/google-deepmind\n",
            "6. 0.0205: https://labs.google/\n",
            "7. 0.0190: https://developer.nvidia.com/deep-learning\n",
            "8. 0.0190: https://paperswithcode.com\n",
            "9. 0.0165: https://ai.google/\n",
            "10. 0.0094: https://research.google\n",
            "\n",
            "Query: 'transformer models' (HITS authority ranking)\n",
            "1. 0.1770: https://policies.google.com/terms\n",
            "2. 0.1759: https://policies.google.com/privacy\n",
            "3. 0.1748: https://about.google/\n",
            "4. 0.1746: https://labs.google/\n",
            "5. 0.1721: https://ai.google/\n",
            "6. 0.1663: https://research.google\n",
            "7. 0.1649: https://deepmind.google\n",
            "8. 0.1640: https://github.com/google-deepmind\n",
            "9. 0.1640: https://www.linkedin.com/company/googledeepmind/\n",
            "10. 0.1638: https://ai.google/advancing-ai/milestones/\n",
            "\n",
            "Query: 'reinforcement learning' (PageRank ranking)\n",
            "1. 0.0716: https://github.com/google-deepmind\n",
            "2. 0.0531: https://policies.google.com/terms\n",
            "3. 0.0468: https://about.google/\n",
            "4. 0.0407: https://www.cornell.edu/\n",
            "5. 0.0358: https://www.linkedin.com/company/googledeepmind/\n",
            "6. 0.0223: https://arxiv.org\n",
            "7. 0.0194: https://about.google/products/\n",
            "8. 0.0190: https://developer.nvidia.com/deep-learning\n",
            "9. 0.0190: https://paperswithcode.com\n",
            "10. 0.0102: https://deepmind.com/research/projects/\n",
            "\n",
            "Query: 'reinforcement learning' (HITS authority ranking)\n",
            "1. 0.3280: https://github.com/google-deepmind\n",
            "2. 0.3191: https://deepmind.com/research/projects/\n",
            "3. 0.1770: https://policies.google.com/terms\n",
            "4. 0.1748: https://about.google/\n",
            "5. 0.1732: https://about.google/products/\n",
            "6. 0.1663: https://research.google\n",
            "7. 0.1640: https://www.linkedin.com/company/googledeepmind/\n",
            "8. 0.1595: https://deepmind.com/research/\n",
            "9. 0.1595: https://deepmind.com/research/publications/\n",
            "10. 0.0411: https://cloud.google.com/vertex-ai\n",
            "\n",
            "Query: 'natural language processing' (PageRank ranking)\n",
            "1. 0.1003: https://policies.google.com/privacy\n",
            "2. 0.0716: https://www.linkedin.com/company/googledeepmind/\n",
            "3. 0.0571: https://developer.nvidia.com/deep-learning\n",
            "4. 0.0531: https://policies.google.com/terms\n",
            "5. 0.0447: https://arxiv.org\n",
            "6. 0.0423: https://info.arxiv.org/help/submit/index.html\n",
            "7. 0.0381: https://paperswithcode.com\n",
            "8. 0.0358: https://github.com/google-deepmind\n",
            "9. 0.0282: https://research.google\n",
            "10. 0.0271: https://info.arxiv.org/help/policies/privacy_policy.html\n",
            "\n",
            "Query: 'natural language processing' (HITS authority ranking)\n",
            "1. 0.4989: https://research.google\n",
            "2. 0.4786: https://deepmind.com/models/gemma/gemma-3/\n",
            "3. 0.3517: https://policies.google.com/privacy\n",
            "4. 0.3280: https://www.linkedin.com/company/googledeepmind/\n",
            "5. 0.3191: https://deepmind.com/models/project-mariner/\n",
            "6. 0.3191: https://deepmind.com/models/gemini/flash/\n",
            "7. 0.3191: https://deepmind.com/research/projects/\n",
            "8. 0.3191: https://deepmind.com/models/imagen/\n",
            "9. 0.3191: https://deepmind.com/models/gemini/pro/\n",
            "10. 0.3191: https://deepmind.com/models/gemma/gemma-3n/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnibfVPWOUOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}